{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS MODEL\n",
    "\n",
    "## 0. IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.core.display import HTML, display\n",
    "from IPython.display import display, Audio\n",
    "\n",
    "import sys\n",
    "import base64\n",
    "import struct \n",
    "import os\n",
    "import fnmatch\n",
    "import re\n",
    "\n",
    "import scipy.io.wavfile as wav\n",
    "import python_speech_features as p\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DATA\n",
    "\n",
    "assume data is already converted from NIST to RIF format .wav files. If not run the mfcc_data_convert script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datapath = \"/home/rob/Dropbox/UCL/DIS/Admin/LDC/timit/\"\n",
    "target = datapath+\"TIMIT/\"\n",
    "\n",
    "train_list_wavs, train_list_trans, train_list_mfcc, train_list_fin = [],[],[],[]\n",
    "valid_list_wavs, valid_list_trans, valid_list_mfcc, valid_list_fin = [],[],[],[]\n",
    "test_list_wavs,  test_list_trans,  test_list_mfcc,  test_list_fin  = [],[],[],[]\n",
    "\n",
    "file_count = 0\n",
    "\n",
    "#token = re.compile(\"[\\w-]+|'m|'t|'ll|'ve|'d|'s|\\'\")\n",
    "def clean(word):\n",
    "    ## LC ALL & strip fullstop, comma and semi-colon which are not required\n",
    "    new = word.lower().replace('.','')\n",
    "    new = new.replace(',','')\n",
    "    new = new.replace(';','')\n",
    "    new = new.replace('\"','')\n",
    "    new = new.replace('!','')\n",
    "    new = new.replace('?','')\n",
    "    new = new.replace(':','')\n",
    "    new = new.replace('-','')\n",
    "    return new\n",
    "\n",
    "def read_text(full_wav):\n",
    "    #need to remove _rif.wav (8chars) then add .TXT     \n",
    "    trans_file = full_wav[:-8]+\".TXT\"\n",
    "    with open(trans_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            split  = line.split()\n",
    "            start  = split[0]\n",
    "            end    = split[1]\n",
    "            t_list = split[2:]\n",
    "            trans  = \"\"\n",
    "        # insert cleaned word (lowercase plus removed bad punct)\n",
    "        for t in t_list:\n",
    "            trans = trans+' '+clean(t)\n",
    "    \n",
    "    return start, end, trans\n",
    "\n",
    "for root, dirnames, filenames in os.walk(target):\n",
    "    for filename in fnmatch.filter(filenames, \"*.wav\"):\n",
    "        \n",
    "        full_wav = os.path.join(root, filename)\n",
    "                    \n",
    "## Load file\n",
    "#         fs, audio = wav.read(full_wav)\n",
    "#         mfcc = p.mfcc(audio, samplerate=fs, numcep=26) # produces time x cep\n",
    "        \n",
    "        _, end, trans = read_text(full_wav)\n",
    "    \n",
    "        if 'train' in full_wav.lower():\n",
    "            train_list_wavs.append(full_wav)\n",
    "            train_list_trans.append(trans)\n",
    "            train_list_fin.append(end) \n",
    "\n",
    "        elif 'test' in full_wav.lower():\n",
    "            ##split 50/50 into validation and test (note not random)\n",
    "            if file_count % 2 == 0:\n",
    "                test_list_wavs.append(full_wav)\n",
    "                test_list_trans.append(trans)            \n",
    "                test_list_fin.append(end) \n",
    "            else:\n",
    "                valid_list_wavs.append(full_wav)\n",
    "                valid_list_trans.append(trans)           \n",
    "                valid_list_fin.append(end) \n",
    "        else:\n",
    "            raise IOError\n",
    "\n",
    "        file_count=file_count+1\n",
    "        \n",
    "a = {'wavs' : train_list_wavs,\n",
    "     'fin'  : train_list_fin, \n",
    "     'trans': train_list_trans}\n",
    "\n",
    "b = {'wavs' : valid_list_wavs,\n",
    "     'fin'  : valid_list_fin,\n",
    "     'trans': valid_list_trans}\n",
    "\n",
    "c = {'wavs' : test_list_wavs,\n",
    "     'fin'  : test_list_fin,\n",
    "     'trans': test_list_trans}\n",
    "\n",
    "df_train = pd.DataFrame(a, columns=['fin','trans','wavs'],dtype=int)\n",
    "df_valid = pd.DataFrame(b, columns=['fin','trans','wavs'],dtype=int)\n",
    "df_test = pd.DataFrame(c, columns=['fin','trans','wavs'],dtype=int)\n",
    "    \n",
    "sortagrad = True\n",
    "if sortagrad:\n",
    "    df_train = df_train.sort_values(by='fin',ascending=True)\n",
    "    df_valid = df_valid.sort_values(by='fin',ascending=True)\n",
    "    df_test = df_test.sort_values(by='fin',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"./df_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have 5 seperate lists so if we iterate through with a counter we'll find the ith value is correct for what we need. However later we will need to sort the data which would break this approach therefore we should consider putting the data into a more managable dataformat. We will also need to transform the data (words/characters) into numbers.\n",
    "\n",
    "Baidu's ba-dls-deepspeech implementation uses a character based implementation https://github.com/baidu-research/ba-dls-deepspeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6300\n",
      "(4620, 840, 840)\n"
     ]
    }
   ],
   "source": [
    "print(len(train_list_wavs)+len(test_list_wavs)+len(valid_list_wavs))\n",
    "print(len(train_list_wavs),len(test_list_wavs),len(valid_list_wavs))\n",
    "#6300\n",
    "#(4620, 840, 840)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CORPUS\n",
    "\n",
    "Before we train and build a model, let's see what the max length of a transcription is and build a list of the corpora for later. \"does society really exist as an entity over and above the agglomeration of man?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('max_trans_charlength:', 80)\n"
     ]
    }
   ],
   "source": [
    "## looks like index 150 is the longest sentence with 80chars so we'll have to pad on this.\n",
    "max_mfcc_index = 0\n",
    "max_mfcc_len = 0\n",
    "max_trans_charlength = 0\n",
    "all_words = []\n",
    "\n",
    "comb = train_list_trans+test_list_trans+valid_list_trans\n",
    "# comb_mfcc = train_list_mfcc+test_list_mfcc+valid_list_mfcc\n",
    "\n",
    "for count,sent in enumerate(comb):\n",
    "    #count length\n",
    "    if len(sent)>max_trans_charlength:\n",
    "        max_trans_charlength = len(sent)\n",
    "    #build vocab\n",
    "    for w in sent.split():\n",
    "        all_words.append(clean(w)) \n",
    "    #check mfcc    \n",
    "#     if(comb_mfcc[count].shape[0]>max_mfcc_len):\n",
    "#         max_mfcc_len=comb_mfcc[count].shape[0]\n",
    "#         max_mfcc_index=count\n",
    "    \n",
    "print(\"max_trans_charlength:\",max_trans_charlength)\n",
    "# print(\"max_mfcc_len:\",max_mfcc_len, \"at comb index:\",max_mfcc_index)\n",
    "\n",
    "#('max_trans_charlength:', 80)\n",
    "#('max_mfcc_len:', 778, 'at comb index:', 541)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Longest MFCC shape = (476, 26)\n",
    "# print(list_of_mfcc[150].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Words:', 54198)\n",
      "('Vocab:', 6145)\n"
     ]
    }
   ],
   "source": [
    "## CORPUS - useful for the LM\n",
    "\n",
    "all_vocab = set(all_words) \n",
    "## do some analysis here on the types of words. E.g. a ? will change the sound of a word a lot. \n",
    "print(\"Words:\",len(all_words))\n",
    "print(\"Vocab:\",len(all_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "# From Baidu ba-dls-deepspeech - https://github.com/baidu-research/ba-dls-deepspeech\n",
    "\n",
    "char_map_str = \"\"\"\n",
    "' 1\n",
    "<SPACE> 2\n",
    "a 3\n",
    "b 4\n",
    "c 5\n",
    "d 6\n",
    "e 7\n",
    "f 8\n",
    "g 9\n",
    "h 10\n",
    "i 11\n",
    "j 12\n",
    "k 13\n",
    "l 14\n",
    "m 15\n",
    "n 16\n",
    "o 17\n",
    "p 18\n",
    "q 19\n",
    "r 20\n",
    "s 21\n",
    "t 22\n",
    "u 23\n",
    "v 24\n",
    "w 25\n",
    "x 26\n",
    "y 27\n",
    "z 28\n",
    "\"\"\"\n",
    "\n",
    "char_map = {}\n",
    "index_map = {}\n",
    "\n",
    "for line in char_map_str.strip().split('\\n'):\n",
    "    ch, index = line.split()\n",
    "    char_map[ch] = int(index)\n",
    "    index_map[int(index)] = ch\n",
    "index_map[2] = ' '\n",
    "\n",
    "def text_to_int_sequence(text):\n",
    "    \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
    "    int_sequence = []\n",
    "    for c in text:\n",
    "        if c == ' ':\n",
    "            ch = char_map['<SPACE>']\n",
    "        else:\n",
    "            ch = char_map[c]\n",
    "        int_sequence.append(ch)\n",
    "    return int_sequence\n",
    "\n",
    "##CHECK\n",
    "max_intseq_length = 0\n",
    "for x in train_list_trans+test_list_trans+valid_list_trans:\n",
    "    try:\n",
    "        y=text_to_int_sequence(x)\n",
    "        if len(y)>max_intseq_length:\n",
    "            max_intseq_length=len(y)\n",
    "    except:\n",
    "        print(\"error at:\",x)\n",
    "        \n",
    "print(max_intseq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(char_map)\n",
    "\n",
    "print(num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. KERAS\n",
    "\n",
    "We can load this mfcc data into a DS1 NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.4\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers import Dense, Activation, Bidirectional, Reshape, Lambda, Input\n",
    "from keras.optimizers import SGD, adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# from keras.utils.data_utils import Sequence\n",
    "\n",
    "from keras.layers.merge import add, concatenate\n",
    "import keras.callbacks\n",
    "\n",
    "print(keras.__version__) ##be careful with 2.0.6 as 2.0.4 tested with CoreML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEPS\n",
    "\n",
    "DONE:\n",
    "0. merge the mfcc TIMIT code into this notebook so that variables are accessible\n",
    "2. Build a generator (or sequence keras 2.0.6) include parameters:\n",
    "2. sorting into smaller batches (sortagrad)\n",
    "\n",
    "TODO:\n",
    "\n",
    "4. language model does it get added here or after as output on own?\n",
    "5. WER calc (take from DS)\n",
    "6. exporting (without weird i/o in coreml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global batch_size\n",
    "batch_size = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_intseq(trans):\n",
    "    #PAD\n",
    "    while(len(trans)<max_intseq_length):\n",
    "        trans=trans+' ' #replace with a space char to pad\n",
    "    t = text_to_int_sequence(trans)\n",
    "    return t\n",
    "\n",
    "def get_mfcc(filename):\n",
    "    fs, audio = wav.read(filename)\n",
    "    r = p.mfcc(audio, samplerate=fs, numcep=26) # 2D array -> timesamples x mfcc_features\n",
    "    t = np.transpose(r) # 2D array ->  mfcc_features x timesamples\n",
    "    X = pad_sequences(t, maxlen=778, dtype='float', padding='post', truncating='post').T\n",
    "    return X # 2D array -> MAXtimesamples x mfcc_features {778 x 26}\n",
    "\n",
    "def get_xsize(val):\n",
    "    return val.shape[0]\n",
    "    \n",
    "def get_ylen(val):\n",
    "    return len(val)\n",
    "\n",
    "class timitWavSeq(keras.callbacks.Callback):\n",
    "        def __init__(self, wavpath, transcript, finish):\n",
    "            \n",
    "            self.wavpath = wavpath\n",
    "            self.transcript = transcript\n",
    "            self.start = np.zeros(len(finish))\n",
    "            self.finish = finish\n",
    "            self.length = self.finish\n",
    "            self.batch_size = batch_size\n",
    "            self.cur_train_index = 0\n",
    "            self.cur_val_index = 0\n",
    "            self.cur_test_index = 0\n",
    "            \n",
    "#         def __len__(self):\n",
    "#             ## returns number of batches in the sequence\n",
    "#             return len(self.wavpath) // self.batch_size\n",
    "        \n",
    "        def get_batch(self,idx):\n",
    "                     \n",
    "            batch_x = self.wavpath[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "            batch_y_trans = self.transcript[idx*self.batch_size:(idx+1)*self.batch_size] \n",
    "            \n",
    "            assert(len(batch_x)==self.batch_size)\n",
    "            assert(len(batch_y_trans)==self.batch_size) \n",
    "            \n",
    "            X_data = np.array([get_mfcc(file_name) for file_name in batch_x]) \n",
    "#             print(\"1. X_data.shape:\",X_data.shape)  # ('1. X_data.shape:', (2, 778, 26))\n",
    "            \n",
    "            assert(X_data.shape == (self.batch_size,778,26))\n",
    "            \n",
    "            labels = np.array([get_intseq(l) for l in batch_y_trans])\n",
    "#             print(\"2. labels.shape:\",labels.shape) # ('2. labels.shape:', (2, 80))\n",
    "            \n",
    "            assert (labels.shape == (self.batch_size,max_intseq_length))\n",
    "            \n",
    "            input_length = np.array([get_xsize(mfcc) for mfcc in X_data])\n",
    "#             print(\"3. input_length:\",input_length.shape) # ('3. input_length:', (2,))\n",
    "            assert (input_length.shape == (self.batch_size,))\n",
    "                  \n",
    "            label_length = np.array([get_ylen(y) for y in labels])\n",
    "#             print(\"4. label_length:\",label_length.shape) # ('4. label_length:', (2,))\n",
    "            assert (label_length.shape == (self.batch_size,))\n",
    "            \n",
    "            inputs = {\n",
    "                 'the_input': X_data,\n",
    "                 'the_labels': labels,\n",
    "                 'input_length': input_length,\n",
    "                 'label_length': label_length\n",
    "                 }\n",
    "\n",
    "            outputs = {'ctc': np.zeros([batch_size])}\n",
    "            \n",
    "            return (inputs, outputs)\n",
    "\n",
    "        def next_train(self):\n",
    "            while 1:\n",
    "                ret = self.get_batch(self.cur_train_index)\n",
    "                self.cur_train_index += self.batch_size\n",
    "                if self.cur_train_index >= len(self.wavpath):\n",
    "                    self.cur_train_index=0\n",
    "                yield ret\n",
    "                \n",
    "        def next_val(self):\n",
    "            while 1:\n",
    "                ret = self.get_batch(self.cur_val_index)\n",
    "                self.cur_val_index += self.batch_size\n",
    "                if self.cur_val_index >= len(self.wavpath):\n",
    "                    self.cur_val_index=0\n",
    "                yield ret\n",
    "                \n",
    "        def next_test(self):\n",
    "            while 1:\n",
    "                ret = self.get_batch(self.cur_test_index)\n",
    "                \n",
    "                self.cur_test_index += self.batch_size\n",
    "                if self.cur_test_index >= len(self.wavpath):\n",
    "                    self.cur_test_index=0\n",
    "                yield ret\n",
    "        \n",
    "        def on_train_begin(self, logs={}):\n",
    "            print(\"train begin\")\n",
    "            \n",
    "        def on_epoch_begin(self, epochs, logs={}):\n",
    "            print(\"on epoch begin\")\n",
    "            \n",
    "        def on_epoch_end(self, epoch, logs={}):\n",
    "            print(\"EPOCH END\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sort_train_fin_list = df_train['fin'].tolist()\n",
    "sort_train_trans_list = df_train['trans'].tolist()\n",
    "sort_train_wav_list = df_train['wavs'].tolist()\n",
    "\n",
    "sort_valid_fin_list = df_valid['fin'].tolist()\n",
    "sort_valid_trans_list = df_valid['trans'].tolist()\n",
    "sort_valid_wav_list = df_valid['wavs'].tolist()\n",
    "\n",
    "sort_test_fin_list = df_test['fin'].tolist()\n",
    "sort_test_trans_list = df_test['trans'].tolist()\n",
    "sort_test_wav_list = df_test['wavs'].tolist()\n",
    "\n",
    "traindata = timitWavSeq(wavpath=sort_train_wav_list, transcript=sort_train_trans_list, finish=sort_train_fin_list)\n",
    "validdata = timitWavSeq(wavpath=sort_valid_wav_list, transcript=sort_valid_trans_list, finish=sort_valid_fin_list)\n",
    "testdata = timitWavSeq(wavpath=sort_test_wav_list, transcript=sort_test_trans_list, finish=sort_test_fin_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define CTC loss\n",
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    ''' from TF: Input requirements\n",
    "    1. sequence_length(b) <= time for all b\n",
    "    2. max(labels.indices(labels.indices[:, 1] == b, 2)) <= sequence_length(b) for all b.\n",
    "    '''\n",
    "\n",
    "    print(labels.shape) #(?, 80)\n",
    "    print(y_pred.shape) #(?, 778, 2048)\n",
    "    print(input_length.shape) #(?, 1)\n",
    "    print(label_length.shape) #(?, 1)\n",
    "    \n",
    "    # the 2 is critical here since the first couple outputs of the RNN\n",
    "    # tend to be garbage:\n",
    "    \n",
    "    #y_pred = y_pred[:, 2:, :] ## want to change this?\n",
    "    \n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. MODEL\n",
    "\n",
    "Let's build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network Params\n",
    "fc_size = 2048\n",
    "rnn_size = 512\n",
    "mfcc_features = 26\n",
    "max_mfcclength_audio = 778\n",
    "\n",
    "X = np.zeros([batch_size, max_mfcclength_audio, mfcc_features])\n",
    "\n",
    "# Creates a tensor there are always 26 MFCC\n",
    "input_data = Input(name='the_input', shape=X.shape[1:]) # >>(?, 778, 26)\n",
    "\n",
    "# First 3 FC layers\n",
    "x = Dense(fc_size, name='fc1', activation='relu')(input_data) # >>(?, 778, 2048)\n",
    "x = Dense(fc_size, name='fc2', activation='relu')(x) # >>(?, 778, 2048)\n",
    "x = Dense(fc_size, name='fc3', activation='relu')(x) # >>(?, 778, 2048)\n",
    "\n",
    "# Layer 4 BiDirectional RNN\n",
    "\n",
    "rnn_1f = SimpleRNN(rnn_size, return_sequences=True, go_backwards=False, \n",
    "                   kernel_initializer='he_normal', name='rnn_f')(x) #>>(?, ?, 512)\n",
    "\n",
    "rnn_1b = SimpleRNN(rnn_size, return_sequences=True, go_backwards=True, \n",
    "                   kernel_initializer='he_normal', name='rnn_b')(x) #>>(?, ?, 512)\n",
    "\n",
    "rnn_merged = add([rnn_1f, rnn_1b]) #>>(?, ?, 512)\n",
    "\n",
    "#TODO TRY THIS FROM: https://github.com/fchollet/keras/issues/2838\n",
    "# rnn_bidir1 = merge([rnn_fwd1, rnn_bwd1], mode='concat')\n",
    "# predictions = TimeDistributed(Dense(output_class_size, activation='softmax'))(rnn_bidir1) \n",
    "\n",
    "x = Activation('relu', name='birelu')(rnn_merged) #>>(?, ?, 512)\n",
    "\n",
    "# Layer 5 FC Layer\n",
    "y_pred = Dense(fc_size, name='fc5', activation='relu')(x) #>>(?, 778, 2048)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 80)\n",
      "(?, 778, 2048)\n",
      "(?, 1)\n",
      "(?, 1)\n"
     ]
    }
   ],
   "source": [
    "# Change shape \n",
    "labels = Input(name='the_labels', shape=[80], dtype='float32')\n",
    "input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "\n",
    "# Keras doesn't currently support loss funcs with extra parameters\n",
    "# so CTC loss is implemented in a lambda layer\n",
    "loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred,\n",
    "                                                                   labels, \n",
    "                                                                   input_length, \n",
    "                                                                   label_length])\n",
    "\n",
    "sgd = SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n",
    "\n",
    "model = Model(inputs=[input_data, labels, input_length, label_length], outputs=loss_out)\n",
    "\n",
    "# the loss calc occurs elsewhere, so use a dummy lambda func for the loss\n",
    "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the output of the model? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Layer (type)              Output Shape      Param #  Connected to               \n",
      "================================================================================\n",
      "the_input (InputLayer)    (None, 778, 26)   0                                   \n",
      "________________________________________________________________________________\n",
      "fc1 (Dense)               (None, 778, 2048) 55296    the_input[0][0]            \n",
      "________________________________________________________________________________\n",
      "fc2 (Dense)               (None, 778, 2048) 4196352  fc1[0][0]                  \n",
      "________________________________________________________________________________\n",
      "fc3 (Dense)               (None, 778, 2048) 4196352  fc2[0][0]                  \n",
      "________________________________________________________________________________\n",
      "rnn_f (SimpleRNN)         (None, 778, 512)  1311232  fc3[0][0]                  \n",
      "________________________________________________________________________________\n",
      "rnn_b (SimpleRNN)         (None, 778, 512)  1311232  fc3[0][0]                  \n",
      "________________________________________________________________________________\n",
      "add_8 (Add)               (None, 778, 512)  0        rnn_f[0][0]                \n",
      "                                                     rnn_b[0][0]                \n",
      "________________________________________________________________________________\n",
      "birelu (Activation)       (None, 778, 512)  0        add_8[0][0]                \n",
      "________________________________________________________________________________\n",
      "fc5 (Dense)               (None, 778, 2048) 1050624  birelu[0][0]               \n",
      "________________________________________________________________________________\n",
      "the_labels (InputLayer)   (None, 80)        0                                   \n",
      "________________________________________________________________________________\n",
      "input_length (InputLayer) (None, 1)         0                                   \n",
      "________________________________________________________________________________\n",
      "label_length (InputLayer) (None, 1)         0                                   \n",
      "________________________________________________________________________________\n",
      "ctc (Lambda)              (None, 1)         0        fc5[0][0]                  \n",
      "                                                     the_labels[0][0]           \n",
      "                                                     input_length[0][0]         \n",
      "                                                     label_length[0][0]         \n",
      "================================================================================\n",
      "Total params: 12,121,088\n",
      "Trainable params: 12,121,088\n",
      "Non-trainable params: 0\n",
      "________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary(line_length=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## 8. TRAIN\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23, 4)\n"
     ]
    }
   ],
   "source": [
    "## Make it smaller for perpose of demo\n",
    "train_steps = len(train_list_wavs)/25//batch_size\n",
    "valid_steps = len(valid_list_wavs)/25//batch_size\n",
    "\n",
    "print(train_steps, valid_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train begin\n",
      "on epoch begin\n",
      "Epoch 1/2\n",
      "22/23 [===========================>..] - ETA: 15s - loss: 1470.8689EPOCH END\n",
      "23/23 [==============================] - 351s - loss: 1420.1190    \n",
      "on epoch begin\n",
      "Epoch 2/2\n",
      "22/23 [===========================>..] - ETA: 14s - loss: 350.6266EPOCH END\n",
      "23/23 [==============================] - 338s - loss: 349.2300    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4f7f98c4d0>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit_generator(generator=traindata.next_train(),\n",
    "                    steps_per_epoch=train_steps, #28\n",
    "                    epochs=2, \n",
    "                    callbacks=[traindata], ##create custom callback to handle stop for valid\n",
    "                    \n",
    "                    validation_data=None,\n",
    "                    validation_steps=None,\n",
    "                    initial_epoch=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUG WITH TF & Batch Size = 1\n",
    "\n",
    "https://github.com/fchollet/keras/issues/6635\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## BIG CTC ERROR \n",
    "\n",
    "#BATCH_SIZE=2\n",
    "InvalidArgumentError (see above for traceback): label SparseTensor is not valid: \n",
    "indices[24] = [0,24] is out of bounds: need 0 <= index < [2,24]\n",
    "\n",
    "#BATCH_SIZE=10\n",
    "InvalidArgumentError (see above for traceback): label SparseTensor is not valid: \n",
    "indices[24] = [0,24] is out of bounds: need 0 <= index < [10,24]\n",
    "    \n",
    "#BATCH_SIZE=32  \n",
    "InvalidArgumentError (see above for traceback): label SparseTensor is not valid:\n",
    "indices[24] = [0,24] is out of bounds: need 0 <= index < [32,24]\n",
    "\n",
    "## DIFFERENT DIMS SWAP [batch_size, mfcc, 778] vs old [batch_size, 778, mfcc]\n",
    "\n",
    "InvalidArgumentError (see above for traceback): sequence_length(0) <= 776\n",
    "\t [[Node: ctc_22/CTCLoss = CTCLoss[ctc_merge_repeated=true, preprocess_collapse_repeated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ctc_22/Log, ctc_22/ToInt64, ctc_22/ToInt32_2, ctc_22/ToInt32_1)]]\n",
    "\n",
    "\n",
    "InvalidArgumentError (see above for traceback): Not enough time for target transition sequence (required: 80, available: 26), skipping data instance in batch: 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/5 [=======================>......] - ETA: 5s "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-32:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python2.7/threading.py\", line 754, in run\n",
      "    self.__target(*self.__args, **self.__kwargs)\n",
      "  File \"/home/rob/py27/local/lib/python2.7/site-packages/keras/engine/training.py\", line 612, in data_generator_task\n",
      "    generator_output = next(self._generator)\n",
      "  File \"<ipython-input-101-aabe4a3ca035>\", line 93, in next_test\n",
      "    ret = self.get_batch(self.cur_test_index)\n",
      "  File \"<ipython-input-101-aabe4a3ca035>\", line 43, in get_batch\n",
      "    assert(len(batch_x)==self.batch_size)\n",
      "AssertionError\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 26s    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 131.88670349],\n",
       "       [ 189.01681519],\n",
       "       [ 282.64770508],\n",
       "       [ 107.13787842],\n",
       "       [ 258.87561035],\n",
       "       [ 236.39028931],\n",
       "       [ 223.16127014],\n",
       "       [ 161.07373047],\n",
       "       [ 281.67977905],\n",
       "       [ 248.79734802],\n",
       "       [ 233.22113037],\n",
       "       [ 281.21868896],\n",
       "       [ 211.15124512],\n",
       "       [ 208.48600769],\n",
       "       [ 216.82899475],\n",
       "       [ 204.44763184],\n",
       "       [ 197.55986023],\n",
       "       [ 238.77764893],\n",
       "       [ 298.02575684],\n",
       "       [ 231.60652161],\n",
       "       [ 415.53540039],\n",
       "       [ 292.26651001],\n",
       "       [ 258.68112183],\n",
       "       [ 235.75234985],\n",
       "       [ 306.95895386],\n",
       "       [ 295.81634521],\n",
       "       [ 280.23269653],\n",
       "       [ 283.27096558],\n",
       "       [ 287.94567871],\n",
       "       [ 326.68359375],\n",
       "       [ 300.39996338],\n",
       "       [ 269.24191284],\n",
       "       [ 282.01095581],\n",
       "       [ 344.5602417 ],\n",
       "       [ 326.61203003],\n",
       "       [ 289.81646729],\n",
       "       [ 362.6890564 ],\n",
       "       [ 298.21893311],\n",
       "       [ 346.61257935],\n",
       "       [ 303.37142944]], dtype=float32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## not sure this is best use of gen / test data creation\n",
    "\n",
    "model.predict_generator( testdata.next_test(), 5, workers=1, verbose=1)\n",
    "#model.evaluate(x, y, batch_size=32, verbose=1, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 9. EXPORT\n",
    "\n",
    "We can export the model ready for CoreML to convert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# serialize model to JSON\n",
    "with open(\"ds_ctc_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model.to_json())\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"ds_ctc_model_weights.h5\")\n",
    "\n",
    "# # save data to .npz\n",
    "# np.savez('xor_data.npz', training_data=training_data, target_data=target_data, test_data=test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. CONVERT\n",
    "\n",
    "Converting is expected to fail here due to the Lambda layer required for the CTC function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import coremltools\n",
    "\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 80)\n",
      "(?, 778, 2048)\n",
      "(?, 1)\n",
      "(?, 1)\n",
      "Loaded model/weights from disk\n",
      "Convert Model\n",
      "Keras layer '<class 'keras.layers.core.Lambda'>' not supported. \n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('ds_ctc_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into loaded model\n",
    "loaded_model.load_weights(\"ds_ctc_model_weights.h5\")\n",
    "print(\"Loaded model/weights from disk\")\n",
    "\n",
    "# it looks like this has worked. We can now convert to \n",
    "print(\"Convert Model\")\n",
    "try:\n",
    "    coreml_model = coremltools.converters.keras.convert(loaded_model)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. REMOVE LAMBDA\n",
    "\n",
    "We are required to remove the lambda layer and use a normal activation function. In theory it would be possible to build a graph that is identical to the tensorflow model and transfer the weights. We still want the majority of the graph to be converted to coreml though.\n",
    "\n",
    "Things to try:\n",
    "\n",
    "1. Use Keras pop layer to remove lambda \n",
    "2. Recreate model without Lambda\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. POP KERAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "### Doesn't work, must be an issue with the outputs. Instead we can transfer to new\n",
    "\n",
    "# 1. Use Keras pop layer to remove lambda\n",
    "\n",
    "# METHOD1 use pop and correct output\n",
    "\n",
    "# print(\"Before\")\n",
    "# loaded_model.layers.pop()\n",
    "# loaded_model.layers.pop()\n",
    "# loaded_model.layers.pop()\n",
    "# loaded_model.layers.pop()\n",
    "\n",
    "# loaded_model.summary(line_length=80)\n",
    "\n",
    "# # Get input\n",
    "# new_input = loaded_model.input\n",
    "# # Find the layer to connect\n",
    "# hidden_layer = loaded_model.layers[-1].output\n",
    "# # Connect a new layer on it\n",
    "# out = Dense(fc_size, name='final_out')(hidden_layer)\n",
    "# # Build a new model\n",
    "# model3 = Model(new_input, out)\n",
    "# # print(\"After\")\n",
    "# model3.summary(line_length=80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert Model\n",
      "Keras layer '<class 'keras.layers.core.Lambda'>' not supported. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# it looks like this has worked. We can now convert to \n",
    "print(\"Convert Model\")\n",
    "\n",
    "try:\n",
    "    coreml_model = coremltools.converters.keras.convert(loaded_model)\n",
    "    # Set model metadata\n",
    "    coreml_model.author = 'Rob Smith'\n",
    "    coreml_model.license = 'BSD'\n",
    "    coreml_model.short_description = 'Performs keras ds '\n",
    "\n",
    "    # SAVE\n",
    "    coreml_model.save('kds.mlmodel')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. REBUILD WITHOUT LAMBDA\n",
    "\n",
    "let's see if we get the same error building from scratch. Also copy over the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network Params\n",
    "fc_size = 2048\n",
    "rnn_size = 512\n",
    "mfcc_features = 26\n",
    "max_mfcclength_audio = 778\n",
    "\n",
    "X = np.zeros([batch_size, max_mfcclength_audio, mfcc_features])\n",
    "\n",
    "# Creates a tensor there are always 26 MFCC\n",
    "input_data = Input(name='the_input', shape=X.shape[1:]) # >>(?, 778, 26)\n",
    "\n",
    "# First 3 FC layers\n",
    "x = Dense(fc_size, name='fc1', activation='relu', \n",
    "          weights=loaded_model.layers[1].get_weights())(input_data) # >>(?, 778, 2048)\n",
    "x = Dense(fc_size, name='fc2', activation='relu',\n",
    "         weights=loaded_model.layers[2].get_weights())(x) # >>(?, 778, 2048)\n",
    "x = Dense(fc_size, name='fc3', activation='relu',\n",
    "         weights=loaded_model.layers[3].get_weights())(x) # >>(?, 778, 2048)\n",
    "\n",
    "# Layer 4 BiDirectional RNN\n",
    "\n",
    "rnn_1f = SimpleRNN(rnn_size, return_sequences=True, go_backwards=False, \n",
    "                   kernel_initializer='he_normal', name='rnn_f',\n",
    "                   weights=loaded_model.layers[4].get_weights())(x) #>>(?, ?, 512)\n",
    "\n",
    "rnn_1b = SimpleRNN(rnn_size, return_sequences=True, go_backwards=True, \n",
    "                   kernel_initializer='he_normal', name='rnn_b',\n",
    "                   weights=loaded_model.layers[5].get_weights())(x) #>>(?, ?, 512)\n",
    "\n",
    "rnn_merged = add([rnn_1f, rnn_1b],\n",
    "                weights=loaded_model.layers[6].get_weights()) #>>(?, ?, 512)\n",
    "x = Activation('relu', name='birelu',\n",
    "              weights=loaded_model.layers[7].get_weights())(rnn_merged) #>>(?, ?, 512)\n",
    "\n",
    "# Layer 5 FC Layer\n",
    "y_pred = Dense(fc_size, name='fc5', activation='relu',\n",
    "              weights=loaded_model.layers[8].get_weights())(x) #>>(?, 778, 2048)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#####################################\n",
    "\n",
    "# Change shape \n",
    "labels = Input(name='the_labels', shape=[80], dtype='float32')\n",
    "input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "\n",
    "# Keras doesn't currently support loss funcs with extra parameters\n",
    "# so CTC loss is implemented in a lambda layer\n",
    "# loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred,\n",
    "#                                                                    labels, \n",
    "#                                                                    input_length, \n",
    "#                                                                    label_length])\n",
    "\n",
    "# sgd = SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n",
    "\n",
    "# model = Model(inputs=[input_data, labels, input_length, label_length], outputs=loss_out)\n",
    "\n",
    "# # the loss calc occurs elsewhere, so use a dummy lambda func for the loss\n",
    "# model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=sgd)\n",
    "\n",
    "\n",
    "out = Dense(fc_size, name='final_out')(y_pred)\n",
    "sgd = SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n",
    "model3 = Model(inputs=input_data, outputs=out)\n",
    "\n",
    "model3.compile(loss='mean_squared_error',\n",
    "               optimizer=sgd)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert Model\n",
      "0 : the_input, <keras.engine.topology.InputLayer object at 0x7f4f64b37d90>\n",
      "1 : fc1, <keras.layers.core.Dense object at 0x7f4f64ac9f10>\n",
      "2 : fc1__activation__, <keras.layers.core.Activation object at 0x7f4f66c385d0>\n",
      "3 : fc2, <keras.layers.core.Dense object at 0x7f4f64ac9ed0>\n",
      "4 : fc2__activation__, <keras.layers.core.Activation object at 0x7f4f64ac98d0>\n",
      "5 : fc3, <keras.layers.core.Dense object at 0x7f4f66c38f50>\n",
      "6 : fc3__activation__, <keras.layers.core.Activation object at 0x7f4f64ac9c50>\n",
      "7 : rnn_f, <keras.layers.recurrent.SimpleRNN object at 0x7f4f64accf50>\n",
      "8 : rnn_b, <keras.layers.recurrent.SimpleRNN object at 0x7f4f64a91e50>\n",
      "9 : add_11, <keras.layers.merge.Add object at 0x7f4f64a69a50>\n",
      "10 : birelu, <keras.layers.core.Activation object at 0x7f4f64a7bc90>\n",
      "11 : fc5, <keras.layers.core.Dense object at 0x7f4f6292af50>\n",
      "12 : fc5__activation__, <keras.layers.core.Activation object at 0x7f4f64ac9bd0>\n",
      "13 : final_out, <keras.layers.core.Dense object at 0x7f4f64b37a90>\n"
     ]
    }
   ],
   "source": [
    "print(\"Convert Model\")\n",
    "coreml_model = coremltools.converters.keras.convert(model3)\n",
    "\n",
    "# Set model metadata\n",
    "coreml_model.author = 'Rob Smith'\n",
    "coreml_model.license = 'BSD'\n",
    "coreml_model.short_description = 'Performs keras ds '\n",
    "\n",
    "# Set feature descriptions manually\n",
    "#coreml_model.input_description['the_input'] = 'Audio input'\n",
    "# coreml_model.input_description['bathrooms'] = 'Number of bathrooms'\n",
    "# coreml_model.input_description['size'] = 'Size (in square feet)'\n",
    "\n",
    "# Set the output descriptions\n",
    "# coreml_model.output_description['out'] = 'Audio transcription'\n",
    "\n",
    "# SAVE\n",
    "coreml_model.save('kds.mlmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCLUSION\n",
    "\n",
    "So it finally was exported as a model however it doesn't have a CTC function. Next step will be to train on data with this model with the CTC function and see how it performs in iOS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
